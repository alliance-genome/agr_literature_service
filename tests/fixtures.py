import math
import shutil
from typing import Generator

import pytest
from agr_literature_service.api.models import (
    ReferencetypeModel,
    ModReferencetypeAssociationModel,
    ModModel,
    initialize,
    drop_open_db_sessions)
from agr_literature_service.api.database.base import Base
from agr_literature_service.api.database.config import SQLALCHEMY_DATABASE_URL
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session
from os import environ, path

from agr_literature_service.lit_processing.data_ingest.post_reference_to_db import post_references
from agr_literature_service.lit_processing.tests.mod_populate_load import populate_test_mods
from agr_literature_service.api.config import config
from agr_literature_service.api.crud.ateam_db_helpers import set_globals


def delete_all_table_content(engine, db_session):
    if environ.get('TEST_CLEANUP') == "true":
        print("***** Deleting test data from all tables *****")
        with engine.begin() as conn:  # Use connection context
            for table in reversed(Base.metadata.sorted_tables):
                if table.fullname != "users":
                    conn.execute(table.delete())  # Use connection for execution
        db_session.commit()  # Commit the transaction


@pytest.fixture
def db() -> Generator[Session, None, None]:
    print("***** Creating DB session *****")
    if "rds.amazonaws.com" in config.PSQL_HOST:
        msg = "***** Warning: not allowed to run test on stage or prod database *****"
        pytest.exit(msg)
    else:
        engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"options": "-c timezone=utc"})

        initialize()
        db_session = sessionmaker(bind=engine, autoflush=True)()  # Create session
        db_session.commit()
        delete_all_table_content(engine, db_session)  # Clean before test starts
        yield db_session
        delete_all_table_content(engine, db_session)  # Clean after test ends
        drop_open_db_sessions(db_session)  # Close any open sessions
        print("***** Closing DB session *****")
        db_session.close()  # Close the session


@pytest.fixture
def cleanup_tmp_files_when_done():
    """
    Deletes temp files generated by scripts under XML_PATH

    The cleanup is happening when the test importing this fixture exits, not when it is called
    """
    yield None
    if environ.get('TEST_CLEANUP') == "true":
        base_path = environ.get('XML_PATH')
        shutil.rmtree(base_path)


@pytest.fixture
def populate_test_mod_reference_types(db):
    populate_test_mods()
    mod_reference_types = {
        'ZFIN': ['Journal', 'Review'],
        'FB': ['book'],
        'WB': ['Journal_article', 'Micropublication'],
        'SGD': ['Journal']
    }
    for mod, reference_types in mod_reference_types.items():
        mod_obj = db.query(ModModel).filter(ModModel.abbreviation == mod).one()
        display_order = 10
        for reference_type in reference_types:
            rt_obj = db.query(ReferencetypeModel).filter(ReferencetypeModel.label == reference_type).one_or_none()
            if rt_obj is None:
                rt_obj = ReferencetypeModel(label=reference_type)
                db.add(rt_obj)
            mod_reference_type_obj = ModReferencetypeAssociationModel(mod=mod_obj, referencetype=rt_obj,
                                                                      display_order=display_order)
            db.add(mod_reference_type_obj)
            display_order = math.ceil((display_order + 1) / 10) * 10
    db.commit()


@pytest.fixture
def load_sanitized_references(populate_test_mod_reference_types):
    json_file_path = path.join(path.dirname(path.abspath(__file__)), "lit_processing", "sample_data",
                               "sanitized_references/")
    post_references(json_path=json_file_path)
    yield None


def search_ancestors_or_descendants_mock(ontology_node, ancestors_or_descendants):  # noqa
    workflow_parent = {
        'ATP:0000172': ['ATP:0000177'],
        'ATP:0000140': ['ATP:0000177'],
        'ATP:0000165': ['ATP:0000177'],
        'ATP:0000161': ['ATP:0000177'],
        'ATP:0000175': ['ATP:0000172'],
        'ATP:0000174': ['ATP:0000172'],
        'ATP:0000173': ['ATP:0000172'],
        'ATP:0000178': ['ATP:0000172'],
        'ATP:0000141': ['ATP:0000140'],
        'ATP:0000142': ['ATP:0000140'],
        'ATP:0000135': ['ATP:0000140'],
        'ATP:0000139': ['ATP:0000140'],
        'ATP:0000134': ['ATP:0000140'],
        'ATP:0000168': ['ATP:0000165'],
        'ATP:0000167': ['ATP:0000165'],
        'ATP:0000170': ['ATP:0000165'],
        'ATP:0000171': ['ATP:0000165'],
        'ATP:0000169': ['ATP:0000165'],
        'ATP:0000166': ['ATP:0000165'],
        'ATP:0000164': ['ATP:0000161'],
        'ATP:0000163': ['ATP:0000161'],
        'ATP:0000162': ['ATP:0000161']
    }
    return workflow_parent[ontology_node]


def load_name_to_atp_and_relationships_mock():
    print("### GLOBAL load_name_to_atp_and_relationships_mock ###")
    workflow_children = {
        'ATP:0000009': ['ATP:0000079', 'ATP:0000080', 'ATP:0000081', 'ATP:0000085', 'ATP:0000086', 'ATP:0000087', 'ATP:0000033'],
        'ATP:0000079': ['ATP:0000082', 'ATP:0000083', 'ATP:0000084'],
        'ATP:0000085': ['ATP:0000034', 'ATP:0000100'],
        'ATP:0000177': ['ATP:0000172', 'ATP:0000140', 'ATP:0000165', 'ATP:0000161'],
        'ATP:0000172': ['ATP:0000175', 'ATP:0000174', 'ATP:0000173', 'ATP:0000178'],
        'ATP:0000140': ['ATP:0000141', 'ATP:0000135', 'ATP:0000139', 'ATP:0000134'],
        'ATP:0000165': ['ATP:0000168', 'ATP:0000167', 'ATP:0000170', 'ATP:0000171', 'ATP:0000169', 'ATP:0000166'],
        'ATP:0000161': ['ATP:0000164', 'ATP:0000163', 'ATP:0000162'],

        'ATP:0000001': ['ATP:0000002'],
        'ATP:0000002': ['ATP:0000015', 'ATP:0000142'],
        'ATP:0000142': ['ATP:0000123'],
        'ATP:0000015': ['ATP:0000068', 'ATP:0000069', 'ATP:0000070'],
        'ATP:0000068': ['ATP:0000071'],

        'ATP:fileupload': ['ATP:0000141', 'ATP:fileuploadinprogress', 'ATP:fileuploadcomplete', 'ATP:fileuploadfailed'],
        'ATP:0000166': ['ATP:task1_needed', 'ATP:task2_needed', 'ATP:task3_needed'],
        'ATP:0000178': ['ATP:task1_in_progress', 'ATP:task2_in_progress', 'ATP:task3_in_progress'],
        'ATP:0000189': ['ATP:task1_failed', 'ATP:task2_failed', 'ATP:task3_failed'],
        'ATP:0000169': ['ATP:task1_complete', 'ATP:task2_complete', 'ATP:task3_complete'],
        'ATP:0000335': ['ATP:0000334', 'ATP:0000321'],
        'ATP:0000321': ['ATP:0000228', 'ATP:0000229']
    }

    atp_to_name = {
        'ATP:0000009': 'phenotype',
        'ATP:0000082': 'RNAi phenotype',
        'ATP:0000122': 'ATP:0000122',
        'ATP:0000084': 'overexpression phenotype',
        'ATP:0000079': 'genetic phenotype',
        'ATP:0000005': 'gene',
        'ATP:0000123': 'not sure',
        'WB:WBGene00003001': 'lin-12',
        'NCBITaxon:6239': 'Caenorhabditis elegans',
        'ATP:0000141': 'file needed',
        'ont1': 'test',
        'ATP:0000168': 'catalytic activity classification complete',
        'ATP:0000335': 'data novelty',
        'ATP:0000334': 'existing data',
        'ATP:0000321': 'new data',
        'ATP:0000228': 'new to database',
        'ATP:0000229': 'new to field'
    }
    name_to_atp = {
        'phenotype': 'ATP:0000009',
        'RNAi phenotype': 'ATP:0000082',
        'ATP:0000122': 'ATP:0000122',
        'overexpression phenotype': 'ATP:0000084',
        'genetic phenotype': 'ATP:0000079',
        'gene': 'ATP:0000005',
        'lin-12': 'WB:WBGene00003001',
        'Caenorhabditis elegans': 'NCBITaxon:6239',
        'not sure': 'ATP:0000123',
        'data novelty': 'ATP:0000335',
        'existing data': 'ATP:0000334',
        'new data': 'ATP:0000321',
        'new to database': 'ATP:0000228',
        'new to field': 'ATP:0000229'
    }
    workflow_parent = {}
    for atp in workflow_children.keys():
        if atp not in atp_to_name:
            atp_to_name[atp] = atp
        if atp not in name_to_atp:
            name_to_atp[atp] = atp
        for atp2 in workflow_children[atp]:
            workflow_parent[atp2] = atp
            if atp2 not in name_to_atp:
                name_to_atp[atp2] = atp2
            if atp2 not in atp_to_name:
                atp_to_name[atp2] = atp2
    set_globals(atp_to_name, name_to_atp, workflow_children, workflow_parent)
